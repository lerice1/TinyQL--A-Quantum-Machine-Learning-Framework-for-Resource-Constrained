{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe5e7073-b811-46e8-ad5d-48526b70d868",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from torch.optim import Adam\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import psutil\n",
    "import gc\n",
    "from functools import partial\n",
    "from copy import deepcopy\n",
    "\n",
    "# Load the existing code from paste.txt\n",
    "def set_seeds(seed=42):\n",
    "    \"\"\"Set seeds for reproducibility\"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "class QuantumAutoencoder:\n",
    "    def __init__(self, n_qubits, latent_qubits, depth=4, params=None):\n",
    "        self.n_qubits = n_qubits\n",
    "        self.latent_qubits = latent_qubits\n",
    "        self.depth = depth\n",
    "        self.dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "        \n",
    "        self.n_params = self._calculate_params()\n",
    "        if params is not None:\n",
    "            self.params = params\n",
    "        else:\n",
    "            self.params = self._initialize_parameters()\n",
    "        \n",
    "        self.encoder = qml.QNode(self._encoder_circuit, self.dev, interface=\"torch\")\n",
    "        self.decoder = qml.QNode(self._decoder_circuit, self.dev, interface=\"torch\")\n",
    "        \n",
    "        # Add importance scores for pruning\n",
    "        self.importance_scores = np.ones_like(self.params)\n",
    "    \n",
    "    def _calculate_params(self):\n",
    "        \"\"\"Calculate total number of parameters\"\"\"\n",
    "        params_per_qubit = 6\n",
    "        params_per_layer = self.n_qubits * params_per_qubit\n",
    "        total_layers = 2 * self.depth\n",
    "        return params_per_layer * total_layers\n",
    "    \n",
    "    def _initialize_parameters(self):\n",
    "        \"\"\"Improved parameter initialization\"\"\"\n",
    "        params = np.zeros(self.n_params)\n",
    "        # Xavier/Glorot initialization scaled to [0, 2π]\n",
    "        scale = np.sqrt(2.0 / (self.n_qubits + self.latent_qubits)) * np.pi\n",
    "        for i in range(self.n_params):\n",
    "            params[i] = np.random.uniform(-scale, scale)\n",
    "        return params\n",
    "    \n",
    "    def _encoder_circuit(self, data, params):\n",
    "        \"\"\"Enhanced encoder circuit\"\"\"\n",
    "        qml.AmplitudeEmbedding(data, wires=range(self.n_qubits), normalize=True)\n",
    "        \n",
    "        param_idx = 0\n",
    "        for d in range(self.depth):\n",
    "            # More rotations per qubit\n",
    "            for i in range(self.n_qubits):\n",
    "                qml.Rot(params[param_idx], params[param_idx + 1], \n",
    "                       params[param_idx + 2], wires=i)\n",
    "                qml.RX(params[param_idx + 3], wires=i)\n",
    "                qml.RY(params[param_idx + 4], wires=i)\n",
    "                qml.RZ(params[param_idx + 5], wires=i)\n",
    "                param_idx += 6\n",
    "            \n",
    "            # Enhanced entanglement pattern\n",
    "            for i in range(self.n_qubits - 1):\n",
    "                qml.CRZ(params[param_idx % self.n_params], wires=[i, i + 1])\n",
    "                qml.CNOT(wires=[i, i + 1])\n",
    "            \n",
    "            if self.n_qubits > 2:\n",
    "                for i in range(0, self.n_qubits - 2, 2):\n",
    "                    qml.CRX(params[(param_idx + 1) % self.n_params], wires=[i, i + 2])\n",
    "                    qml.CNOT(wires=[i, i + 2])\n",
    "        \n",
    "        return qml.state()\n",
    "    \n",
    "    def _decoder_circuit(self, latent_state, params):\n",
    "        \"\"\"Enhanced decoder circuit\"\"\"\n",
    "        qml.QubitStateVector(latent_state, wires=range(self.latent_qubits))\n",
    "        \n",
    "        # Better initialization of non-latent qubits\n",
    "        for i in range(self.latent_qubits, self.n_qubits):\n",
    "            qml.Hadamard(wires=i)\n",
    "            qml.RY(np.pi/4, wires=i)\n",
    "            qml.RZ(np.pi/4, wires=i)\n",
    "        \n",
    "        param_idx = self.n_params // 2\n",
    "        for d in range(self.depth):\n",
    "            for i in range(self.n_qubits):\n",
    "                qml.Rot(params[param_idx], params[param_idx + 1], \n",
    "                       params[param_idx + 2], wires=i)\n",
    "                qml.RX(params[param_idx + 3], wires=i)\n",
    "                qml.RY(params[param_idx + 4], wires=i)\n",
    "                qml.RZ(params[param_idx + 5], wires=i)\n",
    "                param_idx += 6\n",
    "            \n",
    "            for i in range(self.n_qubits - 1):\n",
    "                qml.CRZ(params[param_idx % self.n_params], wires=[i, i + 1])\n",
    "                qml.CNOT(wires=[i, i + 1])\n",
    "            \n",
    "            if self.n_qubits > 2:\n",
    "                for i in range(0, self.n_qubits - 2, 2):\n",
    "                    qml.CRX(params[(param_idx + 1) % self.n_params], wires=[i, i + 2])\n",
    "                    qml.CNOT(wires=[i, i + 2])\n",
    "        \n",
    "        return qml.state()\n",
    "    \n",
    "    def get_latent_state(self, encoded_state):\n",
    "        \"\"\"Improved latent state extraction\"\"\"\n",
    "        if torch.is_tensor(encoded_state):\n",
    "            encoded_state = encoded_state.detach().numpy()\n",
    "        \n",
    "        state_matrix = np.outer(encoded_state, np.conjugate(encoded_state))\n",
    "        dim = 2**self.latent_qubits\n",
    "        reduced_matrix = state_matrix[:dim, :dim]\n",
    "        \n",
    "        eigenvals, eigenvecs = np.linalg.eigh(reduced_matrix)\n",
    "        # Use top 2 eigenvectors with proper weighting\n",
    "        top_k = 2\n",
    "        top_indices = np.argsort(eigenvals)[-top_k:]\n",
    "        weights = eigenvals[top_indices] / np.sum(eigenvals[top_indices])\n",
    "        latent_state = np.sum([w * eigenvecs[:, i] for w, i in zip(weights, top_indices)], axis=0)\n",
    "        return latent_state / np.linalg.norm(latent_state)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        encoded = self.encoder(x, self.params)\n",
    "        latent = self.get_latent_state(encoded)\n",
    "        decoded = self.decoder(latent, self.params)\n",
    "        return decoded\n",
    "\n",
    "def preprocess_data(X):\n",
    "    \"\"\"Enhanced data preprocessing\"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    # Additional normalization step\n",
    "    X_scaled = X_scaled / np.max(np.abs(X_scaled))\n",
    "    return X_scaled / np.sqrt(np.sum(X_scaled**2, axis=1))[:, np.newaxis]\n",
    "\n",
    "# Function to measure memory usage\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get current memory usage in MB\"\"\"\n",
    "    process = psutil.Process()\n",
    "    memory_info = process.memory_info()\n",
    "    return memory_info.rss / (1024 * 1024)  # Convert bytes to MB\n",
    "\n",
    "# Rényi Entropy-based Pruning\n",
    "def calculate_renyi_entropy(param_values, alpha=2.0):\n",
    "    \"\"\"\n",
    "    Calculate the Rényi entropy of parameter values\n",
    "    \n",
    "    Args:\n",
    "        param_values: Parameter values\n",
    "        alpha: Rényi entropy parameter (alpha=1 corresponds to Shannon entropy,\n",
    "               alpha=2 corresponds to collision entropy)\n",
    "    \n",
    "    Returns:\n",
    "        Rényi entropy value\n",
    "    \"\"\"\n",
    "    # Normalize the values to form a probability distribution\n",
    "    abs_values = np.abs(param_values)\n",
    "    prob_dist = abs_values / np.sum(abs_values)\n",
    "    \n",
    "    # Handle zero probabilities by adding a small epsilon\n",
    "    epsilon = 1e-10\n",
    "    prob_dist = prob_dist + epsilon\n",
    "    prob_dist = prob_dist / np.sum(prob_dist)\n",
    "    \n",
    "    if alpha == 1:\n",
    "        # Shannon entropy for alpha=1\n",
    "        entropy = -np.sum(prob_dist * np.log(prob_dist))\n",
    "    else:\n",
    "        # Rényi entropy for alpha≠1\n",
    "        entropy = 1 / (1 - alpha) * np.log(np.sum(prob_dist ** alpha))\n",
    "    \n",
    "    return entropy\n",
    "\n",
    "def calculate_importance_scores(model, X_sample, alpha=2.0):\n",
    "    \"\"\"\n",
    "    Calculate importance scores for each parameter based on Rényi entropy\n",
    "    \n",
    "    Args:\n",
    "        model: QuantumAutoencoder model\n",
    "        X_sample: Sample data for evaluation\n",
    "        alpha: Rényi entropy parameter\n",
    "    \n",
    "    Returns:\n",
    "        Array of importance scores\n",
    "    \"\"\"\n",
    "    n_params = len(model.params)\n",
    "    importance_scores = np.zeros(n_params)\n",
    "    \n",
    "    # Create parameter masks for each parameter\n",
    "    for i in range(n_params):\n",
    "        # Create a copy of the parameters\n",
    "        temp_params = np.copy(model.params)\n",
    "        \n",
    "        # Create parameter perturbations\n",
    "        perturbations = np.linspace(-0.1, 0.1, 5) * np.pi\n",
    "        entropy_values = []\n",
    "        \n",
    "        for perturbation in perturbations:\n",
    "            temp_params[i] = model.params[i] + perturbation\n",
    "            \n",
    "            # Collect outputs for the sample data\n",
    "            outputs = []\n",
    "            for x in X_sample:\n",
    "                model.params = temp_params\n",
    "                decoded = model.forward(x)\n",
    "                if torch.is_tensor(decoded):\n",
    "                    decoded = decoded.detach().numpy()\n",
    "                outputs.append(decoded)\n",
    "            \n",
    "            # Calculate entropy of the output distribution\n",
    "            outputs = np.array(outputs).flatten()\n",
    "            entropy = calculate_renyi_entropy(outputs, alpha=alpha)\n",
    "            entropy_values.append(entropy)\n",
    "        \n",
    "        # Higher entropy variation means the parameter is more important\n",
    "        importance = np.std(entropy_values)\n",
    "        importance_scores[i] = importance\n",
    "    \n",
    "    # Restore original parameters\n",
    "    model.params = np.copy(model.params)\n",
    "    \n",
    "    # Normalize importance scores\n",
    "    importance_scores = importance_scores / np.sum(importance_scores)\n",
    "    return importance_scores\n",
    "\n",
    "def prune_model(model, pruning_ratio=0.5, X_sample=None, alpha=2.0):\n",
    "    \"\"\"\n",
    "    Prune the quantum model based on Rényi entropy importance scores\n",
    "    \n",
    "    Args:\n",
    "        model: QuantumAutoencoder model\n",
    "        pruning_ratio: Ratio of parameters to prune (0.0 to 1.0)\n",
    "        X_sample: Sample data for calculating importance scores\n",
    "        alpha: Rényi entropy parameter\n",
    "        \n",
    "    Returns:\n",
    "        Pruned model\n",
    "    \"\"\"\n",
    "    if X_sample is not None:\n",
    "        # Calculate importance scores\n",
    "        importance_scores = calculate_importance_scores(model, X_sample, alpha)\n",
    "    else:\n",
    "        importance_scores = model.importance_scores\n",
    "    \n",
    "    # Create a pruned model\n",
    "    pruned_model = deepcopy(model)\n",
    "    pruned_model.importance_scores = importance_scores\n",
    "    \n",
    "    # Sort parameters by importance scores\n",
    "    indices = np.argsort(importance_scores)\n",
    "    n_prune = int(len(indices) * pruning_ratio)\n",
    "    \n",
    "    # Zero out the least important parameters\n",
    "    prune_indices = indices[:n_prune]\n",
    "    pruned_model.params[prune_indices] = 0.0\n",
    "    \n",
    "    return pruned_model\n",
    "\n",
    "# Quantization\n",
    "def quantize_model(model, bits=8):\n",
    "    \"\"\"\n",
    "    Quantize model parameters to reduce precision\n",
    "    \n",
    "    Args:\n",
    "        model: QuantumAutoencoder model\n",
    "        bits: Number of bits for quantization (1-32)\n",
    "        \n",
    "    Returns:\n",
    "        Quantized model\n",
    "    \"\"\"\n",
    "    quantized_model = deepcopy(model)\n",
    "    \n",
    "    # Determine the range of parameters\n",
    "    param_min = np.min(model.params)\n",
    "    param_max = np.max(model.params)\n",
    "    param_range = param_max - param_min\n",
    "    \n",
    "    # Calculate the quantization step\n",
    "    levels = 2**bits - 1\n",
    "    step = param_range / levels\n",
    "    \n",
    "    # Quantize the parameters\n",
    "    quantized_params = np.round((model.params - param_min) / step) * step + param_min\n",
    "    quantized_model.params = quantized_params\n",
    "    \n",
    "    # Store quantization info in the model for potential dequantization\n",
    "    quantized_model.quant_info = {\n",
    "        'bits': bits,\n",
    "        'param_min': param_min,\n",
    "        'param_max': param_max,\n",
    "        'levels': levels\n",
    "    }\n",
    "    \n",
    "    return quantized_model\n",
    "\n",
    "def evaluate_model(model, X_test, metrics_only=False):\n",
    "    \"\"\"\n",
    "    Evaluate the model on test data\n",
    "    \n",
    "    Args:\n",
    "        model: QuantumAutoencoder model\n",
    "        X_test: Test data\n",
    "        metrics_only: If True, only return metrics, not predictions\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of evaluation metrics (and predictions if metrics_only=False)\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    losses = []\n",
    "    predictions = []\n",
    "    \n",
    "    memory_before = get_memory_usage()\n",
    "    for x in X_test:\n",
    "        decoded = model.forward(x)\n",
    "        if torch.is_tensor(decoded):\n",
    "            decoded = decoded.detach().numpy()\n",
    "        \n",
    "        # Calculate reconstruction loss\n",
    "        loss = np.mean((np.real(decoded) - x)**2)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        if not metrics_only:\n",
    "            predictions.append(decoded)\n",
    "    \n",
    "    memory_after = get_memory_usage()\n",
    "    inference_time = time.time() - start_time\n",
    "    test_loss = np.mean(losses)\n",
    "    test_accuracy = 1 / (1 + test_loss)\n",
    "    \n",
    "    non_zero_params = np.count_nonzero(model.params)\n",
    "    model_sparsity = 1.0 - (non_zero_params / len(model.params))\n",
    "    \n",
    "    metrics = {\n",
    "        'loss': test_loss,\n",
    "        'accuracy': test_accuracy,\n",
    "        'inference_time': inference_time,\n",
    "        'inference_memory': memory_after - memory_before,\n",
    "        'params_total': len(model.params),\n",
    "        'params_nonzero': non_zero_params,\n",
    "        'sparsity': model_sparsity\n",
    "    }\n",
    "    \n",
    "    if not metrics_only:\n",
    "        metrics['predictions'] = predictions\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def train_and_evaluate_models(n_epochs=100, batch_size=4, learning_rate=0.002, \n",
    "                              pruning_ratio=0.5, quant_bits=8, alpha=2.0, seed=42):\n",
    "    \"\"\"\n",
    "    Train the original model, prune it, quantize it, and evaluate all three\n",
    "    \n",
    "    Args:\n",
    "        n_epochs: Number of training epochs\n",
    "        batch_size: Batch size for training\n",
    "        learning_rate: Learning rate for optimizer\n",
    "        pruning_ratio: Ratio of parameters to prune\n",
    "        quant_bits: Number of bits for quantization\n",
    "        seed: Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of models and results\n",
    "    \"\"\"\n",
    "    set_seeds(seed)\n",
    "    \n",
    "    # Generate synthetic data\n",
    "    n_features = 16\n",
    "    X, y = make_classification(\n",
    "        n_samples=200,\n",
    "        n_features=n_features,\n",
    "        n_classes=2,\n",
    "        n_informative=6,\n",
    "        n_redundant=0,\n",
    "        n_clusters_per_class=2,\n",
    "        class_sep=2.5,\n",
    "        random_state=seed\n",
    "    )\n",
    "    \n",
    "    X_train, X_test = train_test_split(X, test_size=0.2, random_state=seed)\n",
    "    X_train = preprocess_data(X_train)\n",
    "    X_test = preprocess_data(X_test)\n",
    "    \n",
    "    # Initialize the original model\n",
    "    n_qubits = int(np.log2(X_train.shape[1]))\n",
    "    latent_qubits = n_qubits - 1\n",
    "    original_model = QuantumAutoencoder(n_qubits=n_qubits, latent_qubits=latent_qubits)\n",
    "    \n",
    "    # Train the original model\n",
    "    print(\"\\nTraining the original model...\")\n",
    "    train_start_time = time.time()\n",
    "    memory_before_training = get_memory_usage()\n",
    "    \n",
    "    params = torch.tensor(original_model.params, requires_grad=True)\n",
    "    optimizer = Adam([params], lr=learning_rate)\n",
    "    \n",
    "    best_accuracy = 0.0\n",
    "    best_params = None\n",
    "    patience = 10\n",
    "    patience_counter = 0\n",
    "    \n",
    "    metrics = {'train_losses': [], 'train_accuracies': [], \n",
    "              'val_losses': [], 'val_accuracies': []}\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        original_model.params = params.detach().numpy()\n",
    "        epoch_loss = 0\n",
    "        n_batches = max(1, len(X_train) // batch_size)\n",
    "        \n",
    "        for _ in range(n_batches):\n",
    "            optimizer.zero_grad()\n",
    "            batch_idx = np.random.choice(len(X_train), min(batch_size, len(X_train)))\n",
    "            batch_data = X_train[batch_idx]\n",
    "            \n",
    "            total_loss = torch.tensor(0.0, requires_grad=True)\n",
    "            for x in batch_data:\n",
    "                decoded = original_model.forward(x)\n",
    "                if torch.is_tensor(decoded):\n",
    "                    decoded = decoded.real\n",
    "                decoded = torch.tensor(np.real(decoded), dtype=torch.float64, requires_grad=True)\n",
    "                x_tensor = torch.tensor(x, dtype=torch.float32)\n",
    "                \n",
    "                # Enhanced loss calculation\n",
    "                reconstruction_loss = torch.mean((decoded - x_tensor)**2)\n",
    "                l2_reg = 0.0001 * torch.sum(params**2)\n",
    "                loss = reconstruction_loss + l2_reg\n",
    "                \n",
    "                total_loss = total_loss + loss\n",
    "            \n",
    "            avg_loss = total_loss / len(batch_data)\n",
    "            avg_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_([params], max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            epoch_loss += avg_loss.item()\n",
    "        \n",
    "        train_loss = epoch_loss / n_batches\n",
    "        train_accuracy = 1 / (1 + train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for x in X_test:\n",
    "                decoded = original_model.forward(x)\n",
    "                decoded = np.real(decoded) if not torch.is_tensor(decoded) else decoded.real.numpy()\n",
    "                val_loss = np.mean((decoded - x)**2)\n",
    "                val_losses.append(val_loss)\n",
    "        \n",
    "        val_loss = np.mean(val_losses)\n",
    "        val_accuracy = 1 / (1 + val_loss)\n",
    "        \n",
    "        metrics['train_losses'].append(train_loss)\n",
    "        metrics['train_accuracies'].append(train_accuracy)\n",
    "        metrics['val_losses'].append(val_loss)\n",
    "        metrics['val_accuracies'].append(val_accuracy)\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{n_epochs} | Train Loss: {train_loss:.4f} | Val Acc: {val_accuracy:.4f}\")\n",
    "        \n",
    "        if val_accuracy > best_accuracy:\n",
    "            best_accuracy = val_accuracy\n",
    "            best_params = params.detach().clone()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered!\")\n",
    "            break\n",
    "        \n",
    "        if val_accuracy >= 0.95 and train_accuracy >= 0.95:\n",
    "            print(\"Target accuracy achieved!\")\n",
    "            break\n",
    "    \n",
    "    original_model.params = best_params.numpy()\n",
    "    train_time = time.time() - train_start_time\n",
    "    memory_after_training = get_memory_usage()\n",
    "    training_memory = memory_after_training - memory_before_training\n",
    "    \n",
    "    # Evaluate the original model\n",
    "    print(\"\\nEvaluating the original model...\")\n",
    "    original_results = evaluate_model(original_model, X_test)\n",
    "    \n",
    "    # Calculate importance scores for the original model\n",
    "    print(\"\\nCalculating importance scores for pruning...\")\n",
    "    X_sample = X_train[:min(10, len(X_train))]  # Use a small sample for calculating importance\n",
    "    importance_scores = calculate_importance_scores(original_model, X_sample, alpha=alpha)\n",
    "    original_model.importance_scores = importance_scores\n",
    "    \n",
    "    # Create and evaluate the pruned model\n",
    "    print(f\"\\nPruning the model (pruning ratio: {pruning_ratio}, alpha: {alpha})...\")\n",
    "    pruned_model = prune_model(original_model, pruning_ratio=pruning_ratio, alpha=alpha)\n",
    "    \n",
    "    # Fine-tune the pruned model\n",
    "    print(\"\\nFine-tuning the pruned model...\")\n",
    "    pruned_train_start_time = time.time()\n",
    "    pruned_memory_before = get_memory_usage()\n",
    "    \n",
    "    params = torch.tensor(pruned_model.params, requires_grad=True)\n",
    "    optimizer = Adam([params], lr=learning_rate / 2)  # Lower learning rate for fine-tuning\n",
    "    \n",
    "    for epoch in range(n_epochs // 3):  # Fewer epochs for fine-tuning\n",
    "        pruned_model.params = params.detach().numpy()\n",
    "        epoch_loss = 0\n",
    "        n_batches = max(1, len(X_train) // batch_size)\n",
    "        \n",
    "        for _ in range(n_batches):\n",
    "            optimizer.zero_grad()\n",
    "            batch_idx = np.random.choice(len(X_train), min(batch_size, len(X_train)))\n",
    "            batch_data = X_train[batch_idx]\n",
    "            \n",
    "            total_loss = torch.tensor(0.0, requires_grad=True)\n",
    "            for x in batch_data:\n",
    "                decoded = pruned_model.forward(x)\n",
    "                if torch.is_tensor(decoded):\n",
    "                    decoded = decoded.real\n",
    "                decoded = torch.tensor(np.real(decoded), dtype=torch.float64, requires_grad=True)\n",
    "                x_tensor = torch.tensor(x, dtype=torch.float32)\n",
    "                \n",
    "                reconstruction_loss = torch.mean((decoded - x_tensor)**2)\n",
    "                # No regularization for pruned model fine-tuning\n",
    "                loss = reconstruction_loss\n",
    "                \n",
    "                total_loss = total_loss + loss\n",
    "            \n",
    "            avg_loss = total_loss / len(batch_data)\n",
    "            avg_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Re-zero the pruned parameters to maintain sparsity\n",
    "            with torch.no_grad():\n",
    "                prune_mask = (pruned_model.params == 0.0)\n",
    "                params.data[prune_mask] = 0.0\n",
    "            \n",
    "            epoch_loss += avg_loss.item()\n",
    "        \n",
    "        train_loss = epoch_loss / n_batches\n",
    "        \n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"Fine-tune Epoch {epoch+1} | Train Loss: {train_loss:.4f}\")\n",
    "    \n",
    "    pruned_model.params = params.detach().numpy()\n",
    "    pruned_train_time = time.time() - pruned_train_start_time\n",
    "    pruned_memory_after = get_memory_usage()\n",
    "    pruned_training_memory = pruned_memory_after - pruned_memory_before\n",
    "    \n",
    "    print(\"\\nEvaluating the pruned model...\")\n",
    "    pruned_results = evaluate_model(pruned_model, X_test)\n",
    "    pruned_results['training_time'] = pruned_train_time\n",
    "    pruned_results['training_memory'] = pruned_training_memory\n",
    "    \n",
    "    # Create and evaluate the quantized model\n",
    "    print(f\"\\nQuantizing the pruned model (bits: {quant_bits})...\")\n",
    "    quantized_model = quantize_model(pruned_model, bits=quant_bits)\n",
    "    \n",
    "    # Fine-tune the quantized model\n",
    "    print(\"\\nFine-tuning the quantized model...\")\n",
    "    quant_train_start_time = time.time()\n",
    "    quant_memory_before = get_memory_usage()\n",
    "    \n",
    "    params = torch.tensor(quantized_model.params, requires_grad=True)\n",
    "    optimizer = Adam([params], lr=learning_rate / 3)  # Even lower learning rate for quantized model\n",
    "    \n",
    "    for epoch in range(n_epochs // 4):  # Even fewer epochs for fine-tuning\n",
    "        quantized_model.params = params.detach().numpy()\n",
    "        epoch_loss = 0\n",
    "        n_batches = max(1, len(X_train) // batch_size)\n",
    "        \n",
    "        for _ in range(n_batches):\n",
    "            optimizer.zero_grad()\n",
    "            batch_idx = np.random.choice(len(X_train), min(batch_size, len(X_train)))\n",
    "            batch_data = X_train[batch_idx]\n",
    "            \n",
    "            total_loss = torch.tensor(0.0, requires_grad=True)\n",
    "            for x in batch_data:\n",
    "                decoded = quantized_model.forward(x)\n",
    "                if torch.is_tensor(decoded):\n",
    "                    decoded = decoded.real\n",
    "                decoded = torch.tensor(np.real(decoded), dtype=torch.float64, requires_grad=True)\n",
    "                x_tensor = torch.tensor(x, dtype=torch.float32)\n",
    "                \n",
    "                reconstruction_loss = torch.mean((decoded - x_tensor)**2)\n",
    "                loss = reconstruction_loss\n",
    "                \n",
    "                total_loss = total_loss + loss\n",
    "            \n",
    "            avg_loss = total_loss / len(batch_data)\n",
    "            avg_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Re-quantize parameters after each update\n",
    "            with torch.no_grad():\n",
    "                # Keep pruned parameters at zero\n",
    "                prune_mask = (pruned_model.params == 0.0)\n",
    "                params.data[prune_mask] = 0.0\n",
    "                \n",
    "                # Re-quantize non-zero parameters\n",
    "                param_min = quantized_model.quant_info['param_min']\n",
    "                param_max = quantized_model.quant_info['param_max']\n",
    "                levels = quantized_model.quant_info['levels']\n",
    "                step = (param_max - param_min) / levels\n",
    "                \n",
    "                # Clip to the range\n",
    "                params.data = torch.clamp(params.data, param_min, param_max)\n",
    "                \n",
    "                # Quantize\n",
    "                params.data = torch.round((params.data - param_min) / step) * step + param_min\n",
    "            \n",
    "            epoch_loss += avg_loss.item()\n",
    "        \n",
    "        train_loss = epoch_loss / n_batches\n",
    "        \n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"Quant Fine-tune Epoch {epoch+1} | Train Loss: {train_loss:.4f}\")\n",
    "    \n",
    "    quantized_model.params = params.detach().numpy()\n",
    "    quant_train_time = time.time() - quant_train_start_time\n",
    "    quant_memory_after = get_memory_usage()\n",
    "    quant_training_memory = quant_memory_after - quant_memory_before\n",
    "    \n",
    "    print(\"\\nEvaluating the quantized model...\")\n",
    "    quantized_results = evaluate_model(quantized_model, X_test)\n",
    "    quantized_results['training_time'] = quant_train_time\n",
    "    quantized_results['training_memory'] = quant_training_memory\n",
    "    \n",
    "    # Add training time and memory to original results\n",
    "    original_results['training_time'] = train_time\n",
    "    original_results['training_memory'] = training_memory\n",
    "    \n",
    "    # Compare models\n",
    "    print(\"\\n=== Model Comparison ===\")\n",
    "    print(f\"{'Model':<10} | {'Accuracy':<10} | {'Train Time':<12} | {'Infer Time':<12} | {'Train Mem (MB)':<15} | {'Infer Mem (MB)':<15} | {'Parameters':<12} | {'Sparsity':<10}\")\n",
    "    print(\"-\" * 110)\n",
    "    \n",
    "    print(f\"{'Original':<10} | {original_results['accuracy']:<10.4f} | {original_results['training_time']:<12.4f}s | {original_results['inference_time']:<12.4f}s | {original_results['training_memory']:<15.2f} | {original_results['inference_memory']:<15.2f} | {original_results['params_nonzero']}/{original_results['params_total']} | {original_results['sparsity']:<10.2%}\")\n",
    "    \n",
    "    print(f\"{'Pruned':<10} | {pruned_results['accuracy']:<10.4f} | {pruned_results['training_time']:<12.4f}s | {pruned_results['inference_time']:<12.4f}s | {pruned_results['training_memory']:<15.2f} | {pruned_results['inference_memory']:<15.2f} | {pruned_results['params_nonzero']}/{pruned_results['params_total']} | {pruned_results['sparsity']:<10.2%}\")\n",
    "    \n",
    "    print(f\"{'Quantized':<10} | {quantized_results['accuracy']:<10.4f} | {quantized_results['training_time']:<12.4f}s | {quantized_results['inference_time']:<12.4f}s | {quantized_results['training_memory']:<15.2f} | {quantized_results['inference_memory']:<15.2f} | {quantized_results['params_nonzero']}/{quantized_results['params_total']} | {quantized_results['sparsity']:<10.2%}\")\n",
    "    \n",
    "    # Calculate memory and time savings\n",
    "    mem_saving_pruned = (original_results['inference_memory'] - pruned_results['inference_memory']) / original_results['inference_memory'] * 100\n",
    "    mem_saving_quantized = (original_results['inference_memory'] - quantized_results['inference_memory']) / original_results['inference_memory'] * 100\n",
    "    \n",
    "    time_saving_pruned = (original_results['inference_time'] - pruned_results['inference_time']) / original_results['inference_time'] * 100\n",
    "    time_saving_quantized = (original_results['inference_time'] - quantized_results['inference_time']) / original_results['inference_time'] * 100\n",
    "    \n",
    "    accuracy_change_pruned = (pruned_results['accuracy'] - original_results['accuracy']) / original_results['accuracy'] * 100\n",
    "    accuracy_change_quantized = (quantized_results['accuracy'] - original_results['accuracy']) / original_results['accuracy'] * 100\n",
    "    \n",
    "    print(\"\\n=== Savings Summary ===\")\n",
    "    print(f\"Pruned Model Memory Savings: {mem_saving_pruned:.2f}%\")\n",
    "    print(f\"Pruned Model Time Savings: {time_saving_pruned:.2f}%\")\n",
    "    print(f\"Pruned Model Accuracy Change: {accuracy_change_pruned:.2f}%\")\n",
    "    print(f\"Quantized Model Memory Savings: {mem_saving_quantized:.2f}%\")\n",
    "    print(f\"Quantized Model Time Savings: {time_saving_quantized:.2f}%\")\n",
    "    print(f\"Quantized Model Accuracy Change: {accuracy_change_quantized:.2f}%\")\n",
    "    \n",
    "    # Plot metrics for visualization\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Plot accuracy comparison\n",
    "    plt.subplot(2, 2, 1)\n",
    "    models = ['Original', 'Pruned', 'Quantized']\n",
    "    accuracies = [original_results['accuracy'], pruned_results['accuracy'], quantized_results['accuracy']]\n",
    "    plt.bar(models, accuracies, color=['blue', 'green', 'orange'])\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Model Accuracy Comparison')\n",
    "    plt.ylim(0.5, 1.0)  # Set y-axis limits for better visualization\n",
    "    \n",
    "    # Plot inference time comparison\n",
    "    plt.subplot(2, 2, 2)\n",
    "    inference_times = [original_results['inference_time'], pruned_results['inference_time'], quantized_results['inference_time']]\n",
    "    plt.bar(models, inference_times, color=['blue', 'green', 'orange'])\n",
    "    plt.ylabel('Inference Time (s)')\n",
    "    plt.title('Inference Time Comparison')\n",
    "    \n",
    "    # Plot inference memory comparison\n",
    "    plt.subplot(2, 2, 3)\n",
    "    inference_memory = [original_results['inference_memory'], pruned_results['inference_memory'], quantized_results['inference_memory']]\n",
    "    plt.bar(models, inference_memory, color=['blue', 'green', 'orange'])\n",
    "    plt.ylabel('Inference Memory (MB)')\n",
    "    plt.title('Inference Memory Comparison')\n",
    "    \n",
    "    # Plot parameter counts\n",
    "    plt.subplot(2, 2, 4)\n",
    "    params_nonzero = [original_results['params_nonzero'], pruned_results['params_nonzero'], quantized_results['params_nonzero']]\n",
    "    plt.bar(models, params_nonzero, color=['blue', 'green', 'orange'])\n",
    "    plt.ylabel('Non-zero Parameters')\n",
    "    plt.title('Model Parameters Comparison')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('model_comparison.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Return all models and their results\n",
    "    return {\n",
    "        'original': {\n",
    "            'model': original_model,\n",
    "            'results': original_results\n",
    "        },\n",
    "        'pruned': {\n",
    "            'model': pruned_model,\n",
    "            'results': pruned_results\n",
    "        },\n",
    "        'quantized': {\n",
    "            'model': quantized_model,\n",
    "            'results': quantized_results\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf2cace-5270-4e4b-8461-03b2a2ae0c81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6650ef3-8d92-4cd5-8c87-9758d8d0e2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "def visualize_renyi_entropy_heatmap(model, X_sample, alpha_range=[0.5, 1.0, 2.0, 3.0, 4.0]):\n",
    "    \"\"\"\n",
    "    Create a heatmap showing how Rényi entropy changes across different alpha values\n",
    "    \n",
    "    Args:\n",
    "        model: QuantumAutoencoder model\n",
    "        X_sample: Sample data for evaluation\n",
    "        alpha_range: Range of alpha values to test\n",
    "    \"\"\"\n",
    "    n_params = len(model.params)\n",
    "    n_to_show = min(100, n_params)  # Show at most 100 parameters for clarity\n",
    "    \n",
    "    # Select parameter indices\n",
    "    if n_params > n_to_show:\n",
    "        indices = np.linspace(0, n_params-1, n_to_show, dtype=int)\n",
    "    else:\n",
    "        indices = np.arange(n_params)\n",
    "    \n",
    "    # Create a matrix to store entropy values\n",
    "    entropy_matrix = np.zeros((len(alpha_range), len(indices)))\n",
    "    \n",
    "    # Calculate entropy for each alpha and parameter\n",
    "    for i, alpha in enumerate(alpha_range):\n",
    "        for j, param_idx in enumerate(indices):\n",
    "            # Create a copy of the parameters\n",
    "            temp_params = np.copy(model.params)\n",
    "            \n",
    "            # Zero out the parameter\n",
    "            temp_params[param_idx] = 0.0\n",
    "            \n",
    "            # Collect outputs with zero parameter\n",
    "            outputs = []\n",
    "            model.params = temp_params\n",
    "            for x in X_sample:\n",
    "                decoded = model.forward(x)\n",
    "                if hasattr(decoded, 'detach'):\n",
    "                    decoded = decoded.detach().numpy()\n",
    "                outputs.append(decoded)\n",
    "            \n",
    "            # Calculate entropy of the output distribution\n",
    "            outputs = np.array(outputs).flatten()\n",
    "            \n",
    "            # Normalize the values to form a probability distribution\n",
    "            abs_values = np.abs(outputs)\n",
    "            prob_dist = abs_values / np.sum(abs_values)\n",
    "            \n",
    "            # Handle zero probabilities by adding a small epsilon\n",
    "            epsilon = 1e-10\n",
    "            prob_dist = prob_dist + epsilon\n",
    "            prob_dist = prob_dist / np.sum(prob_dist)\n",
    "            \n",
    "            if alpha == 1:\n",
    "                # Shannon entropy for alpha=1\n",
    "                entropy = -np.sum(prob_dist * np.log(prob_dist))\n",
    "            else:\n",
    "                # Rényi entropy for alpha≠1\n",
    "                entropy = 1 / (1 - alpha) * np.log(np.sum(prob_dist ** alpha))\n",
    "            \n",
    "            entropy_matrix[i, j] = entropy\n",
    "    \n",
    "    # Restore original parameters\n",
    "    model.params = np.copy(model.params)\n",
    "    \n",
    "    # Create the heatmap\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Create a custom colormap from blue to red\n",
    "    colors = [(0, 0, 1), (1, 1, 1), (1, 0, 0)]  # Blue -> White -> Red\n",
    "    cmap = LinearSegmentedColormap.from_list(\"entropy_cmap\", colors, N=100)\n",
    "    \n",
    "    # Normalize the entropy values to [-1, 1] for better visualization\n",
    "    entropy_norm = 2 * (entropy_matrix - np.min(entropy_matrix)) / (np.max(entropy_matrix) - np.min(entropy_matrix)) - 1\n",
    "    \n",
    "    # Plot the heatmap\n",
    "    plt.imshow(entropy_norm, cmap=cmap, aspect='auto')\n",
    "    plt.colorbar(label='Normalized Entropy')\n",
    "    plt.xlabel('Parameter Index')\n",
    "    plt.ylabel('Alpha Value')\n",
    "    plt.title('Rényi Entropy Across Alpha Values and Parameters')\n",
    "    \n",
    "    # Set y-ticks to alpha values\n",
    "    plt.yticks(np.arange(len(alpha_range)), [f\"{alpha:.1f}\" for alpha in alpha_range])\n",
    "    \n",
    "    # Set x-ticks to parameter indices\n",
    "    plt.xticks(np.arange(0, len(indices), max(1, len(indices)//10)), \n",
    "              [f\"{idx}\" for idx in indices[::max(1, len(indices)//10)]])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('renyi_entropy_heatmap.png')\n",
    "    plt.close()\n",
    "\n",
    "def plot_circuit_diagram(model, filename='quantum_circuit.png'):\n",
    "    \"\"\"\n",
    "    Plot the quantum circuit diagram of the model\n",
    "    \n",
    "    Args:\n",
    "        model: QuantumAutoencoder model\n",
    "        filename: Output filename for the circuit diagram\n",
    "    \"\"\"\n",
    "    import pennylane as qml\n",
    "    \n",
    "    # Create a new figure\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Get a sample input\n",
    "    x_sample = np.random.random(2**model.n_qubits)\n",
    "    x_sample = x_sample / np.linalg.norm(x_sample)\n",
    "    \n",
    "    # Create a circuit diagram for the encoder\n",
    "    encoder_fig, encoder_ax = qml.draw_mpl(model.encoder, expansion_strategy=\"device\")(x_sample, model.params)\n",
    "    encoder_ax.set_title(\"Encoder Circuit\")\n",
    "    \n",
    "    # Save the encoder circuit diagram\n",
    "    encoder_fig.tight_layout()\n",
    "    encoder_fig.savefig('encoder_circuit.png')\n",
    "    \n",
    "    # Create a latent state\n",
    "    encoded_state = model.encoder(x_sample, model.params)\n",
    "    latent_state = model.get_latent_state(encoded_state)\n",
    "    \n",
    "    # Create a circuit diagram for the decoder\n",
    "    decoder_fig, decoder_ax = qml.draw_mpl(model.decoder, expansion_strategy=\"device\")(latent_state, model.params)\n",
    "    decoder_ax.set_title(\"Decoder Circuit\")\n",
    "    \n",
    "    # Save the decoder circuit diagram\n",
    "    decoder_fig.tight_layout()\n",
    "    decoder_fig.savefig('decoder_circuit.png')\n",
    "    \n",
    "    # Combined image for both encoder and decoder\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.imshow(plt.imread('encoder_circuit.png'))\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.imshow(plt.imread('decoder_circuit.png'))\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "    \n",
    "    return filename\n",
    "\n",
    "def visualize_model_size_comparison(original_results, pruned_results, quantized_results):\n",
    "    \"\"\"\n",
    "    Visualize the model size comparison between original, pruned, and quantized models\n",
    "    \n",
    "    Args:\n",
    "        original_results: Results dictionary for the original model\n",
    "        pruned_results: Results dictionary for the pruned model\n",
    "        quantized_results: Results dictionary for the quantized model\n",
    "    \"\"\"\n",
    "    # Calculate model sizes\n",
    "    param_size_original = original_results['params_total'] * 8  # 8 bytes for float64\n",
    "    param_size_pruned = pruned_results['params_nonzero'] * 8  # 8 bytes for float64\n",
    "    param_size_quantized = quantize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9311d20b-9352-408a-9ab6-de7ae5df3290",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d7a286-acf6-4ab3-9e15-b7d228463874",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984c710f-cb7f-402a-8f30-cfa933d6bbbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e6912f-20c8-4fe5-a240-70852bfdc7a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da20318-30de-4aea-88e4-51acf722faee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60c74f4-5203-49d1-8d99-a6af4068ee19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d263433-6386-4c15-9a03-f1cad5be3e2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf17702c-3113-4170-9ca0-438b62df046e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758fa16b-568f-4134-b1e8-cf565e738e75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3960faef-e8e7-48e6-81af-bb1920162754",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'quantum_model_optimization'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Import the quantum model optimization code\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mquantum_model_optimization\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Import visualization helpers\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mvisualization_helper\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'quantum_model_optimization'"
     ]
    }
   ],
   "source": [
    "import pennylane as qml\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from torch.optim import Adam\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import psutil\n",
    "import gc\n",
    "from functools import partial\n",
    "from copy import deepcopy\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Import the quantum model optimization code\n",
    "from quantum_model_optimization import *\n",
    "\n",
    "# Import visualization helpers\n",
    "from visualization_helper import *\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the complete model optimization pipeline\n",
    "    \"\"\"\n",
    "    # Set random seed for reproducibility\n",
    "    SEED = 42\n",
    "    set_seeds(SEED)\n",
    "    \n",
    "    # Create a directory for results if it doesn't exist\n",
    "    results_dir = \"optimization_results\"\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    os.chdir(results_dir)\n",
    "    \n",
    "    # Configuration parameters\n",
    "    config = {\n",
    "        'n_epochs': 100,\n",
    "        'batch_size': 4,\n",
    "        'learning_rate': 0.002,\n",
    "        'pruning_ratio': 0.5,\n",
    "        'quant_bits': 8,\n",
    "        'alpha': 2.0,\n",
    "        'seed': SEED,\n",
    "    }\n",
    "    \n",
    "    print(\"=== Quantum Model Optimization Pipeline ===\")\n",
    "    print(\"\\nConfiguration:\")\n",
    "    for key, value in config.items():\n",
    "        print(f\"- {key}: {value}\")\n",
    "    \n",
    "    # Train and evaluate models\n",
    "    print(\"\\nRunning main optimization pipeline...\")\n",
    "    results = train_and_evaluate_models(\n",
    "        n_epochs=config['n_epochs'],\n",
    "        batch_size=config['batch_size'],\n",
    "        learning_rate=config['learning_rate'],\n",
    "        pruning_ratio=config['pruning_ratio'],\n",
    "        quant_bits=config['quant_bits'],\n",
    "        alpha=config['alpha'],\n",
    "        seed=config['seed']\n",
    "    )\n",
    "    \n",
    "    # Extract models\n",
    "    original_model = results['original']['model']\n",
    "    pruned_model = results['pruned']['model']\n",
    "    quantized_model = results['quantized']['model']\n",
    "    \n",
    "    # Extract results\n",
    "    original_results = results['original']['results']\n",
    "    pruned_results = results['pruned']['results']\n",
    "    quantized_results = results['quantized']['results']\n",
    "    \n",
    "    # Generate data for additional analysis\n",
    "    n_features = 16\n",
    "    X, y = make_classification(\n",
    "        n_samples=200,\n",
    "        n_features=n_features,\n",
    "        n_classes=2,\n",
    "        n_informative=6,\n",
    "        n_redundant=0,\n",
    "        n_clusters_per_class=2,\n",
    "        class_sep=2.5,\n",
    "        random_state=SEED\n",
    "    )\n",
    "    \n",
    "    X_train, X_test = train_test_split(X, test_size=0.2, random_state=SEED)\n",
    "    X_train = preprocess_data(X_train)\n",
    "    X_test = preprocess_data(X_test)\n",
    "    \n",
    "    # Additional visualizations\n",
    "    print(\"\\nGenerating parameter distribution visualization...\")\n",
    "    visualize_parameter_distribution(original_model, pruned_model, quantized_model)\n",
    "    \n",
    "    print(\"\\nGenerating importance vs value visualization...\")\n",
    "    visualize_importance_vs_value(original_model)\n",
    "    \n",
    "    print(\"\\nVisualizing model size comparison...\")\n",
    "    visualize_model_size_comparison(original_results, pruned_results, quantized_results)\n",
    "    \n",
    "    print(\"\\nVisualizing parameter importance...\")\n",
    "    visualize_parameter_importance(original_model)\n",
    "    \n",
    "    print(\"\\nVisualizing latent space...\")\n",
    "    visualize_latent_space(original_model, X_test)\n",
    "    \n",
    "    print(\"\\nVisualizing reconstruction quality...\")\n",
    "    visualize_reconstruction_quality(original_model, pruned_model, quantized_model, X_test)\n",
    "    \n",
    "    print(\"\\nGenerating Rényi entropy heatmap...\")\n",
    "    X_sample = X_train[:min(10, len(X_train))]\n",
    "    visualize_renyi_entropy_heatmap(original_model, X_sample)\n",
    "    \n",
    "    try:\n",
    "        print(\"\\nGenerating circuit diagrams...\")\n",
    "        plot_circuit_diagram(original_model)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not generate circuit diagrams. Error: {str(e)}\")\n",
    "    \n",
    "    # Sensitivity analyses\n",
    "    print(\"\\nRunning pruning sensitivity analysis...\")\n",
    "    pruning_sensitivity = run_pruning_sensitivity_analysis(\n",
    "        original_model=original_model,\n",
    "        X_train=X_train,\n",
    "        X_test=X_test,\n",
    "        pruning_ratios=[0.3, 0.5, 0.7, 0.9]\n",
    "    )\n",
    "    \n",
    "    print(\"\\nRunning alpha sensitivity analysis...\")\n",
    "    alpha_sensitivity = run_alpha_sensitivity_analysis(\n",
    "        original_model=original_model,\n",
    "        X_train=X_train,\n",
    "        X_test=X_test,\n",
    "        alphas=[0.5, 1.0, 2.0, 4.0]\n",
    "    )\n",
    "    \n",
    "    print(\"\\nRunning quantization sensitivity analysis...\")\n",
    "    quant_sensitivity = run_quantization_sensitivity_analysis(\n",
    "        pruned_model=pruned_model,\n",
    "        X_test=X_test,\n",
    "        bits_list=[4, 8, 12, 16, 24, 32]\n",
    "    )\n",
    "    \n",
    "    # Create summary report\n",
    "    print(\"\\nCreating summary report...\")\n",
    "    create_comparison_report(\n",
    "        original_results=original_results,\n",
    "        pruned_results=pruned_results,\n",
    "        quantized_results=quantized_results,\n",
    "        pruning_ratio=config['pruning_ratio'],\n",
    "        quant_bits=config['quant_bits'],\n",
    "        alpha=config['alpha']\n",
    "    )\n",
    "    \n",
    "    print(\"\\n=== Optimization Pipeline Completed ===\")\n",
    "    print(f\"All results and visualizations have been saved to: {results_dir}\")\n",
    "    print(\"\\nSummary of improvements:\")\n",
    "    \n",
    "    # Calculate improvements\n",
    "    accuracy_change_pruned = (pruned_results['accuracy'] - original_results['accuracy']) / original_results['accuracy'] * 100\n",
    "    accuracy_change_quantized = (quantized_results['accuracy'] - original_results['accuracy']) / original_results['accuracy'] * 100\n",
    "    \n",
    "    time_saving_pruned = (original_results['inference_time'] - pruned_results['inference_time']) / original_results['inference_time'] * 100\n",
    "    time_saving_quantized = (original_results['inference_time'] - quantized_results['inference_time']) / original_results['inference_time'] * 100\n",
    "    \n",
    "    mem_saving_pruned = (original_results['inference_memory'] - pruned_results['inference_memory']) / original_results['inference_memory'] * 100\n",
    "    mem_saving_quantized = (original_results['inference_memory'] - quantized_results['inference_memory']) / original_results['inference_memory'] * 100\n",
    "    \n",
    "    print(f\"Pruned Model:\")\n",
    "    print(f\"  - Accuracy Change: {accuracy_change_pruned:+.2f}%\")\n",
    "    print(f\"  - Inference Time Savings: {time_saving_pruned:+.2f}%\")\n",
    "    print(f\"  - Memory Usage Savings: {mem_saving_pruned:+.2f}%\")\n",
    "    print(f\"  - Parameter Reduction: {pruned_results['sparsity']:.2%}\")\n",
    "    \n",
    "    print(f\"\\nQuantized Model:\")\n",
    "    print(f\"  - Accuracy Change: {accuracy_change_quantized:+.2f}%\")\n",
    "    print(f\"  - Inference Time Savings: {time_saving_quantized:+.2f}%\")\n",
    "    print(f\"  - Memory Usage Savings: {mem_saving_quantized:+.2f}%\")\n",
    "    print(f\"  - Effective Memory Reduction: {(pruned_results['sparsity'] + (1-pruned_results['sparsity'])*(1-config['quant_bits']/64)):.2%}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82951a0c-8833-4dce-9d1e-df5ff04a059f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342ca361-e890-4f14-81db-0928b761d8e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2f8592a-fc4b-48fa-8f48-fbcdf9498ab3",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (761780950.py, line 321)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 321\u001b[1;36m\u001b[0m\n\u001b[1;33m    return 'model_comparison_report.md'import numpy as np\u001b[0m\n\u001b[1;37m                                       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def visualize_latent_space(model, X_test, n_samples=100):\n",
    "    \"\"\"\n",
    "    Visualize the latent space representation of the test data\n",
    "    \n",
    "    Args:\n",
    "        model: QuantumAutoencoder model\n",
    "        X_test: Test data\n",
    "        n_samples: Number of samples to visualize\n",
    "        \n",
    "    Returns:\n",
    "        Path to the saved figure\n",
    "    \"\"\"\n",
    "    # Use a subset of test data for visualization\n",
    "    n_samples = min(n_samples, len(X_test))\n",
    "    X_subset = X_test[:n_samples]\n",
    "    \n",
    "    # Get encoded states and latent representations\n",
    "    encoded_states = []\n",
    "    latent_states = []\n",
    "    \n",
    "    for x in X_subset:\n",
    "        encoded = model.encoder(x, model.params)\n",
    "        if hasattr(encoded, 'detach'):\n",
    "            encoded = encoded.detach().numpy()\n",
    "        latent = model.get_latent_state(encoded)\n",
    "        \n",
    "        encoded_states.append(encoded)\n",
    "        latent_states.append(latent)\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    encoded_states = np.array(encoded_states)\n",
    "    latent_states = np.array(latent_states)\n",
    "    \n",
    "    # Compute PCA of the latent states for 2D visualization\n",
    "    from sklearn.decomposition import PCA\n",
    "    \n",
    "    # For encoded states\n",
    "    pca_encoded = PCA(n_components=2)\n",
    "    encoded_2d = pca_encoded.fit_transform(np.real(encoded_states.reshape(n_samples, -1)))\n",
    "    \n",
    "    # For latent states\n",
    "    pca_latent = PCA(n_components=2)\n",
    "    latent_2d = pca_latent.fit_transform(np.real(latent_states.reshape(n_samples, -1)))\n",
    "    \n",
    "    # Create a figure for visualization\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    # Plot encoded states PCA\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(encoded_2d[:, 0], encoded_2d[:, 1], c=range(n_samples), cmap='viridis', alpha=0.8)\n",
    "    plt.colorbar(label='Sample Index')\n",
    "    plt.title('PCA of Encoded States')\n",
    "    plt.xlabel('PC1')\n",
    "    plt.ylabel('PC2')\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    # Plot latent states PCA\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(latent_2d[:, 0], latent_2d[:, 1], c=range(n_samples), cmap='viridis', alpha=0.8)\n",
    "    plt.colorbar(label='Sample Index')\n",
    "    plt.title('PCA of Latent States')\n",
    "    plt.xlabel('PC1')\n",
    "    plt.ylabel('PC2')\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('latent_space_visualization.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return 'latent_space_visualization.png'\n",
    "\n",
    "def visualize_reconstruction_quality(original_model, pruned_model, quantized_model, X_test, n_samples=5):\n",
    "    \"\"\"\n",
    "    Visualize reconstruction quality comparison between original, pruned, and quantized models\n",
    "    \n",
    "    Args:\n",
    "        original_model: Original quantum autoencoder model\n",
    "        pruned_model: Pruned model\n",
    "        quantized_model: Quantized model\n",
    "        X_test: Test data\n",
    "        n_samples: Number of samples to visualize\n",
    "        \n",
    "    Returns:\n",
    "        Path to the saved figure\n",
    "    \"\"\"\n",
    "    # Use a subset of test data for visualization\n",
    "    n_samples = min(n_samples, len(X_test))\n",
    "    X_subset = X_test[:n_samples]\n",
    "    \n",
    "    # Get reconstructions from each model\n",
    "    original_recon = []\n",
    "    pruned_recon = []\n",
    "    quantized_recon = []\n",
    "    \n",
    "    for x in X_subset:\n",
    "        # Original model reconstruction\n",
    "        orig_decoded = original_model.forward(x)\n",
    "        if hasattr(orig_decoded, 'detach'):\n",
    "            orig_decoded = orig_decoded.detach().numpy()\n",
    "        original_recon.append(np.real(orig_decoded))\n",
    "        \n",
    "        # Pruned model reconstruction\n",
    "        pruned_decoded = pruned_model.forward(x)\n",
    "        if hasattr(pruned_decoded, 'detach'):\n",
    "            pruned_decoded = pruned_decoded.detach().numpy()\n",
    "        pruned_recon.append(np.real(pruned_decoded))\n",
    "        \n",
    "        # Quantized model reconstruction\n",
    "        quant_decoded = quantized_model.forward(x)\n",
    "        if hasattr(quant_decoded, 'detach'):\n",
    "            quant_decoded = quant_decoded.detach().numpy()\n",
    "        quantized_recon.append(np.real(quant_decoded))\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    original_recon = np.array(original_recon)\n",
    "    pruned_recon = np.array(pruned_recon)\n",
    "    quantized_recon = np.array(quantized_recon)\n",
    "    \n",
    "    # Create a figure for visualization\n",
    "    plt.figure(figsize=(15, 3*n_samples))\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Original input\n",
    "        plt.subplot(n_samples, 4, i*4 + 1)\n",
    "        plt.stem(X_subset[i], use_line_collection=True)\n",
    "        plt.title('Original Input' if i == 0 else '')\n",
    "        plt.ylim(-1.1, 1.1)\n",
    "        if i == n_samples - 1:\n",
    "            plt.xlabel('Feature Index')\n",
    "        \n",
    "        # Original model reconstruction\n",
    "        plt.subplot(n_samples, 4, i*4 + 2)\n",
    "        plt.stem(original_recon[i], use_line_collection=True)\n",
    "        plt.title('Original Model' if i == 0 else '')\n",
    "        plt.ylim(-1.1, 1.1)\n",
    "        if i == n_samples - 1:\n",
    "            plt.xlabel('Feature Index')\n",
    "        \n",
    "        # Pruned model reconstruction\n",
    "        plt.subplot(n_samples, 4, i*4 + 3)\n",
    "        plt.stem(pruned_recon[i], use_line_collection=True)\n",
    "        plt.title('Pruned Model' if i == 0 else '')\n",
    "        plt.ylim(-1.1, 1.1)\n",
    "        if i == n_samples - 1:\n",
    "            plt.xlabel('Feature Index')\n",
    "        \n",
    "        # Quantized model reconstruction\n",
    "        plt.subplot(n_samples, 4, i*4 + 4)\n",
    "        plt.stem(quantized_recon[i], use_line_collection=True)\n",
    "        plt.title('Quantized Model' if i == 0 else '')\n",
    "        plt.ylim(-1.1, 1.1)\n",
    "        if i == n_samples - 1:\n",
    "            plt.xlabel('Feature Index')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('reconstruction_quality.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return 'reconstruction_quality.png'\n",
    "\n",
    "def visualize_parameter_importance(model, top_n=20):\n",
    "    \"\"\"\n",
    "    Visualize the most important parameters based on their importance scores\n",
    "    \n",
    "    Args:\n",
    "        model: Model with importance scores\n",
    "        top_n: Number of top important parameters to visualize\n",
    "        \n",
    "    Returns:\n",
    "        Path to the saved figure\n",
    "    \"\"\"\n",
    "    # Sort parameters by importance\n",
    "    indices = np.argsort(model.importance_scores)[::-1]\n",
    "    top_indices = indices[:top_n]\n",
    "    top_importance = model.importance_scores[top_indices]\n",
    "    top_values = model.params[top_indices]\n",
    "    \n",
    "    # Create a figure for visualization\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Plot importance scores\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.bar(range(top_n), top_importance, color='purple', alpha=0.7)\n",
    "    plt.title('Top Parameter Importance Scores')\n",
    "    plt.xlabel('Parameter Rank')\n",
    "    plt.ylabel('Importance Score')\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    # Plot parameter values\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.bar(range(top_n), top_values, color='green', alpha=0.7)\n",
    "    plt.title('Top Parameter Values')\n",
    "    plt.xlabel('Parameter Rank')\n",
    "    plt.ylabel('Parameter Value')\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('parameter_importance.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Also create a scatter plot of importance vs. value\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(np.abs(model.params), model.importance_scores, alpha=0.5)\n",
    "    plt.title('Parameter Importance vs. Magnitude')\n",
    "    plt.xlabel('Parameter Magnitude (absolute value)')\n",
    "    plt.ylabel('Importance Score')\n",
    "    plt.yscale('log')  # Log scale for better visualization\n",
    "    plt.colorbar(plt.cm.ScalarMappable(norm=plt.Normalize(vmin=0, vmax=1)))\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig('importance_vs_magnitude.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return 'parameter_importance.png', 'importance_vs_magnitude.png'\n",
    "\n",
    "def create_comparison_report(original_results, pruned_results, quantized_results, \n",
    "                             pruning_ratio, quant_bits, alpha):\n",
    "    \"\"\"\n",
    "    Create a summary report comparing the performance of all models\n",
    "    \n",
    "    Args:\n",
    "        original_results: Results dictionary for the original model\n",
    "        pruned_results: Results dictionary for the pruned model\n",
    "        quantized_results: Results dictionary for the quantized model\n",
    "        pruning_ratio: Pruning ratio used\n",
    "        quant_bits: Quantization bits used\n",
    "        alpha: Rényi entropy parameter used\n",
    "        \n",
    "    Returns:\n",
    "        Path to the saved report\n",
    "    \"\"\"\n",
    "    # Calculate improvements/reductions\n",
    "    accuracy_change_pruned = (pruned_results['accuracy'] - original_results['accuracy']) / original_results['accuracy'] * 100\n",
    "    accuracy_change_quantized = (quantized_results['accuracy'] - original_results['accuracy']) / original_results['accuracy'] * 100\n",
    "    \n",
    "    time_saving_pruned = (original_results['inference_time'] - pruned_results['inference_time']) / original_results['inference_time'] * 100\n",
    "    time_saving_quantized = (original_results['inference_time'] - quantized_results['inference_time']) / original_results['inference_time'] * 100\n",
    "    \n",
    "    mem_saving_pruned = (original_results['inference_memory'] - pruned_results['inference_memory']) / original_results['inference_memory'] * 100\n",
    "    mem_saving_quantized = (original_results['inference_memory'] - quantized_results['inference_memory']) / original_results['inference_memory'] * 100\n",
    "    \n",
    "    # Create a report as a Markdown file\n",
    "    with open('model_comparison_report.md', 'w') as f:\n",
    "        f.write(\"# Quantum Autoencoder Optimization Report\\n\\n\")\n",
    "        \n",
    "        f.write(\"## Optimization Configuration\\n\")\n",
    "        f.write(f\"- Pruning Ratio: {pruning_ratio:.2f}\\n\")\n",
    "        f.write(f\"- Quantization Bits: {quant_bits}\\n\")\n",
    "        f.write(f\"- Rényi Entropy Alpha: {alpha:.2f}\\n\\n\")\n",
    "        \n",
    "        f.write(\"## Performance Metrics\\n\\n\")\n",
    "        \n",
    "        f.write(\"### Accuracy\\n\")\n",
    "        f.write(f\"- Original Model: {original_results['accuracy']:.4f}\\n\")\n",
    "        f.write(f\"- Pruned Model: {pruned_results['accuracy']:.4f} ({accuracy_change_pruned:+.2f}%)\\n\")\n",
    "        f.write(f\"- Quantized Model: {quantized_results['accuracy']:.4f} ({accuracy_change_quantized:+.2f}%)\\n\\n\")\n",
    "        \n",
    "        f.write(\"### Inference Time\\n\")\n",
    "        f.write(f\"- Original Model: {original_results['inference_time']:.4f}s\\n\")\n",
    "        f.write(f\"- Pruned Model: {pruned_results['inference_time']:.4f}s ({time_saving_pruned:+.2f}%)\\n\")\n",
    "        f.write(f\"- Quantized Model: {quantized_results['inference_time']:.4f}s ({time_saving_quantized:+.2f}%)\\n\\n\")\n",
    "        \n",
    "        f.write(\"### Memory Usage\\n\")\n",
    "        f.write(f\"- Original Model: {original_results['inference_memory']:.2f} MB\\n\")\n",
    "        f.write(f\"- Pruned Model: {pruned_results['inference_memory']:.2f} MB ({mem_saving_pruned:+.2f}%)\\n\")\n",
    "        f.write(f\"- Quantized Model: {quantized_results['inference_memory']:.2f} MB ({mem_saving_quantized:+.2f}%)\\n\\n\")\n",
    "        \n",
    "        f.write(\"### Model Parameters\\n\")\n",
    "        f.write(f\"- Original Model: {original_results['params_nonzero']}/{original_results['params_total']} parameters (Sparsity: {original_results['sparsity']:.2%})\\n\")\n",
    "        f.write(f\"- Pruned Model: {pruned_results['params_nonzero']}/{pruned_results['params_total']} parameters (Sparsity: {pruned_results['sparsity']:.2%})\\n\")\n",
    "        f.write(f\"- Quantized Model: {quantized_results['params_nonzero']}/{quantized_results['params_total']} parameters (Sparsity: {quantized_results['sparsity']:.2%})\\n\\n\")\n",
    "        \n",
    "        f.write(\"## Visualization\\n\\n\")\n",
    "        f.write(\"Several visualizations have been generated to help analyze the models:\\n\\n\")\n",
    "        f.write(\"1. `model_comparison.png`: Bar charts comparing accuracy, inference time, and memory usage\\n\")\n",
    "        f.write(\"2. `parameter_distributions.png`: Histograms of parameter distributions for all models\\n\")\n",
    "        f.write(\"3. `importance_vs_value.png`: Scatter plot of parameter importance vs. parameter value\\n\")\n",
    "        f.write(\"4. `pruning_sensitivity.png`: Analysis of model performance across different pruning ratios\\n\")\n",
    "        f.write(\"5. `alpha_sensitivity.png`: Analysis of model performance across different alpha values\\n\")\n",
    "        f.write(\"6. `quantization_sensitivity.png`: Analysis of model performance across different bit depths\\n\")\n",
    "        f.write(\"7. `reconstruction_quality.png`: Comparison of reconstruction quality between models\\n\")\n",
    "        f.write(\"8. `latent_space_visualization.png`: PCA visualization of the latent space\\n\")\n",
    "        f.write(\"9. `renyi_entropy_heatmap.png`: Heatmap of Rényi entropy across parameters and alpha values\\n\\n\")\n",
    "        \n",
    "        f.write(\"## Conclusion\\n\\n\")\n",
    "        \n",
    "        # Overall assessment\n",
    "        overall_assessment = \"The optimization process \"\n",
    "        if accuracy_change_quantized >= -1:  # Less than 1% accuracy loss\n",
    "            overall_assessment += \"successfully maintained model accuracy while \"\n",
    "        elif accuracy_change_quantized >= -5:  # Less than 5% accuracy loss\n",
    "            overall_assessment += \"resulted in minimal accuracy loss while \"\n",
    "        else:\n",
    "            overall_assessment += \"resulted in some accuracy degradation but \"\n",
    "            \n",
    "        if time_saving_quantized > 30 or mem_saving_quantized > 30:\n",
    "            overall_assessment += \"significantly reducing computational resources.\"\n",
    "        elif time_saving_quantized > 10 or mem_saving_quantized > 10:\n",
    "            overall_assessment += \"moderately reducing computational resources.\"\n",
    "        else:\n",
    "            overall_assessment += \"yielding some resource efficiency improvements.\"\n",
    "            \n",
    "        f.write(overall_assessment + \"\\n\\n\")\n",
    "        \n",
    "        # Specific recommendations\n",
    "        f.write(\"### Recommendations\\n\\n\")\n",
    "        \n",
    "        if accuracy_change_pruned > accuracy_change_quantized:\n",
    "            f.write(\"- The pruned model provides better accuracy-efficiency trade-off than the quantized model.\\n\")\n",
    "        else:\n",
    "            f.write(\"- The quantized model provides better accuracy-efficiency trade-off than the pruned model.\\n\")\n",
    "            \n",
    "        if accuracy_change_quantized < -5:\n",
    "            f.write(\"- Consider using a lower pruning ratio or higher bit depth to preserve more accuracy.\\n\")\n",
    "            \n",
    "        if time_saving_quantized < 10 and mem_saving_quantized < 10:\n",
    "            f.write(\"- Explore more aggressive optimization techniques as current savings are modest.\\n\")\n",
    "            \n",
    "        f.write(\"- For deployment scenarios where memory is the primary constraint, the quantized model is recommended.\\n\")\n",
    "        f.write(\"- For deployment scenarios where inference speed is critical, the pruned model may be preferred.\\n\")\n",
    "    \n",
    "    return 'model_comparison_report.md'import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "def visualize_renyi_entropy_heatmap(model, X_sample, alpha_range=[0.5, 1.0, 2.0, 3.0, 4.0]):\n",
    "    \"\"\"\n",
    "    Create a heatmap showing how Rényi entropy changes across different alpha values\n",
    "    \n",
    "    Args:\n",
    "        model: QuantumAutoencoder model\n",
    "        X_sample: Sample data for evaluation\n",
    "        alpha_range: Range of alpha values to test\n",
    "    \"\"\"\n",
    "    n_params = len(model.params)\n",
    "    n_to_show = min(100, n_params)  # Show at most 100 parameters for clarity\n",
    "    \n",
    "    # Select parameter indices\n",
    "    if n_params > n_to_show:\n",
    "        indices = np.linspace(0, n_params-1, n_to_show, dtype=int)\n",
    "    else:\n",
    "        indices = np.arange(n_params)\n",
    "    \n",
    "    # Create a matrix to store entropy values\n",
    "    entropy_matrix = np.zeros((len(alpha_range), len(indices)))\n",
    "    \n",
    "    # Calculate entropy for each alpha and parameter\n",
    "    for i, alpha in enumerate(alpha_range):\n",
    "        for j, param_idx in enumerate(indices):\n",
    "            # Create a copy of the parameters\n",
    "            temp_params = np.copy(model.params)\n",
    "            \n",
    "            # Zero out the parameter\n",
    "            temp_params[param_idx] = 0.0\n",
    "            \n",
    "            # Collect outputs with zero parameter\n",
    "            outputs = []\n",
    "            model.params = temp_params\n",
    "            for x in X_sample:\n",
    "                decoded = model.forward(x)\n",
    "                if hasattr(decoded, 'detach'):\n",
    "                    decoded = decoded.detach().numpy()\n",
    "                outputs.append(decoded)\n",
    "            \n",
    "            # Calculate entropy of the output distribution\n",
    "            outputs = np.array(outputs).flatten()\n",
    "            \n",
    "            # Normalize the values to form a probability distribution\n",
    "            abs_values = np.abs(outputs)\n",
    "            prob_dist = abs_values / np.sum(abs_values)\n",
    "            \n",
    "            # Handle zero probabilities by adding a small epsilon\n",
    "            epsilon = 1e-10\n",
    "            prob_dist = prob_dist + epsilon\n",
    "            prob_dist = prob_dist / np.sum(prob_dist)\n",
    "            \n",
    "            if alpha == 1:\n",
    "                # Shannon entropy for alpha=1\n",
    "                entropy = -np.sum(prob_dist * np.log(prob_dist))\n",
    "            else:\n",
    "                # Rényi entropy for alpha≠1\n",
    "                entropy = 1 / (1 - alpha) * np.log(np.sum(prob_dist ** alpha))\n",
    "            \n",
    "            entropy_matrix[i, j] = entropy\n",
    "    \n",
    "    # Restore original parameters\n",
    "    model.params = np.copy(model.params)\n",
    "    \n",
    "    # Create the heatmap\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Create a custom colormap from blue to red\n",
    "    colors = [(0, 0, 1), (1, 1, 1), (1, 0, 0)]  # Blue -> White -> Red\n",
    "    cmap = LinearSegmentedColormap.from_list(\"entropy_cmap\", colors, N=100)\n",
    "    \n",
    "    # Normalize the entropy values to [-1, 1] for better visualization\n",
    "    entropy_norm = 2 * (entropy_matrix - np.min(entropy_matrix)) / (np.max(entropy_matrix) - np.min(entropy_matrix)) - 1\n",
    "    \n",
    "    # Plot the heatmap\n",
    "    plt.imshow(entropy_norm, cmap=cmap, aspect='auto')\n",
    "    plt.colorbar(label='Normalized Entropy')\n",
    "    plt.xlabel('Parameter Index')\n",
    "    plt.ylabel('Alpha Value')\n",
    "    plt.title('Rényi Entropy Across Alpha Values and Parameters')\n",
    "    \n",
    "    # Set y-ticks to alpha values\n",
    "    plt.yticks(np.arange(len(alpha_range)), [f\"{alpha:.1f}\" for alpha in alpha_range])\n",
    "    \n",
    "    # Set x-ticks to parameter indices\n",
    "    plt.xticks(np.arange(0, len(indices), max(1, len(indices)//10)), \n",
    "              [f\"{idx}\" for idx in indices[::max(1, len(indices)//10)]])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('renyi_entropy_heatmap.png')\n",
    "    plt.close()\n",
    "\n",
    "def plot_circuit_diagram(model, filename='quantum_circuit.png'):\n",
    "    \"\"\"\n",
    "    Plot the quantum circuit diagram of the model\n",
    "    \n",
    "    Args:\n",
    "        model: QuantumAutoencoder model\n",
    "        filename: Output filename for the circuit diagram\n",
    "    \"\"\"\n",
    "    import pennylane as qml\n",
    "    \n",
    "    # Create a new figure\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Get a sample input\n",
    "    x_sample = np.random.random(2**model.n_qubits)\n",
    "    x_sample = x_sample / np.linalg.norm(x_sample)\n",
    "    \n",
    "    # Create a circuit diagram for the encoder\n",
    "    encoder_fig, encoder_ax = qml.draw_mpl(model.encoder, expansion_strategy=\"device\")(x_sample, model.params)\n",
    "    encoder_ax.set_title(\"Encoder Circuit\")\n",
    "    \n",
    "    # Save the encoder circuit diagram\n",
    "    encoder_fig.tight_layout()\n",
    "    encoder_fig.savefig('encoder_circuit.png')\n",
    "    \n",
    "    # Create a latent state\n",
    "    encoded_state = model.encoder(x_sample, model.params)\n",
    "    latent_state = model.get_latent_state(encoded_state)\n",
    "    \n",
    "    # Create a circuit diagram for the decoder\n",
    "    decoder_fig, decoder_ax = qml.draw_mpl(model.decoder, expansion_strategy=\"device\")(latent_state, model.params)\n",
    "    decoder_ax.set_title(\"Decoder Circuit\")\n",
    "    \n",
    "    # Save the decoder circuit diagram\n",
    "    decoder_fig.tight_layout()\n",
    "    decoder_fig.savefig('decoder_circuit.png')\n",
    "    \n",
    "    # Combined image for both encoder and decoder\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.imshow(plt.imread('encoder_circuit.png'))\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.imshow(plt.imread('decoder_circuit.png'))\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "    \n",
    "    return filename\n",
    "\n",
    "def visualize_model_size_comparison(original_results, pruned_results, quantized_results):\n",
    "    \"\"\"\n",
    "    Visualize the model size comparison between original, pruned, and quantized models\n",
    "    \n",
    "    Args:\n",
    "        original_results: Results dictionary for the original model\n",
    "        pruned_results: Results dictionary for the pruned model\n",
    "        quantized_results: Results dictionary for the quantized model\n",
    "    \"\"\"\n",
    "    # Calculate model sizes\n",
    "    param_size_original = original_results['params_total'] * 8  # 8 bytes for float64\n",
    "    param_size_pruned = pruned_results['params_nonzero'] * 8  # 8 bytes for float64\n",
    "    param_size_quantized = quantized_results['params_nonzero'] * (quantized_results.get('quant_info', {}).get('bits', 8) / 8)  # Convert bits to bytes\n",
    "    \n",
    "    # If quantization info is not available, estimate based on typical bit depth\n",
    "    if 'quant_info' not in quantized_results:\n",
    "        param_size_quantized = quantized_results['params_nonzero'] * 1  # Assume 8-bit (1 byte) quantization\n",
    "    \n",
    "    # Calculate memory reduction percentages\n",
    "    pruned_reduction = 100 * (1 - param_size_pruned / param_size_original)\n",
    "    quantized_reduction = 100 * (1 - param_size_quantized / param_size_original)\n",
    "    \n",
    "    # Create a bar chart for model size comparison\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    models = ['Original', 'Pruned', 'Quantized']\n",
    "    sizes = [param_size_original / 1024, param_size_pruned / 1024, param_size_quantized / 1024]  # Convert to KB\n",
    "    \n",
    "    bars = plt.bar(models, sizes, color=['blue', 'green', 'orange'])\n",
    "    \n",
    "    # Add labels and title\n",
    "    plt.ylabel('Model Size (KB)')\n",
    "    plt.title('Model Size Comparison')\n",
    "    \n",
    "    # Add size values and reduction percentages on top of bars\n",
    "    for i, bar in enumerate(bars):\n",
    "        height = bar.get_height()\n",
    "        if i == 0:\n",
    "            plt.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "                    f'{sizes[i]:.2f} KB',\n",
    "                    ha='center', va='bottom')\n",
    "        else:\n",
    "            reduction = pruned_reduction if i == 1 else quantized_reduction\n",
    "            plt.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "                    f'{sizes[i]:.2f} KB\\n(-{reduction:.1f}%)',\n",
    "                    ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('model_size_comparison.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return sizes, [pruned_reduction, quantized_reduction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9ed0f6-c9ba-442a-a094-2b48933ee0f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PeNew",
   "language": "python",
   "name": "penew"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
